{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model approximates the function f(x) = 1/1+e^-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def int_to_64bit_list(number):\n",
    "    binary_str = format(number, '064b')\n",
    "    return [int(bit) for bit in binary_str]\n",
    "\n",
    "def float_to_64bit_list(f):\n",
    "    packed = struct.pack('!d', f)\n",
    "    bits = ''.join(f'{byte:08b}' for byte in packed)\n",
    "    return [int(bit) for bit in bits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "function = lambda x: math.sin(x)\n",
    "\n",
    "ints = [x for x in range(10_000)]\n",
    "\n",
    "input = [int_to_64bit_list(x) for x in ints]\n",
    "\n",
    "output = [float_to_64bit_list(function(x)) for x in ints]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor(input)\n",
    "t[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.labels = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()  \n",
    "        self.layer1 = nn.Linear(64, 256) # 64 bits in\n",
    "        self.layer2 = nn.Linear(256, 256 * 5)  \n",
    "        self.output = nn.Linear(256 * 5, 64) # 64 bits out\n",
    "\n",
    "    def forward(self, x):\n",
    "        # try relu to begin with\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 148.92831420898438\n",
      "Loss: 148.83172607421875\n",
      "Loss: 148.66232299804688\n",
      "Loss: 148.37164306640625\n",
      "Loss: 148.0883331298828\n",
      "Loss: 147.76683044433594\n",
      "Loss: 147.58587646484375\n",
      "Loss: 147.61680603027344\n",
      "Loss: 148.54086303710938\n",
      "Loss: 147.38426208496094\n",
      "Loss: 146.7818603515625\n",
      "Loss: 145.86929321289062\n",
      "Loss: 145.9762420654297\n",
      "Loss: 146.28271484375\n",
      "Loss: 145.29690551757812\n",
      "Loss: 144.73316955566406\n",
      "Loss: 144.54367065429688\n",
      "Loss: 145.49171447753906\n",
      "Loss: 145.05364990234375\n",
      "Loss: 146.30722045898438\n",
      "Loss: 144.94189453125\n",
      "Loss: 145.11029052734375\n",
      "Loss: 143.91091918945312\n",
      "Loss: 145.5323028564453\n",
      "Loss: 147.75991821289062\n",
      "Loss: 145.74375915527344\n",
      "Loss: 143.76748657226562\n",
      "Loss: 142.7989501953125\n",
      "Loss: 143.28746032714844\n",
      "Loss: 143.57847595214844\n",
      "Loss: 143.03887939453125\n",
      "Loss: 141.07485961914062\n",
      "Loss: 141.68145751953125\n",
      "Loss: 142.39599609375\n",
      "Loss: 142.32107543945312\n",
      "Loss: 140.38107299804688\n",
      "Loss: 142.9693145751953\n",
      "Loss: 140.67738342285156\n",
      "Loss: 138.57952880859375\n",
      "Loss: 138.3055877685547\n",
      "Loss: 138.38531494140625\n",
      "Loss: 143.31436157226562\n",
      "Loss: 141.45748901367188\n",
      "Loss: 139.89779663085938\n",
      "Loss: 141.03439331054688\n",
      "Loss: 139.66578674316406\n",
      "Loss: 140.44229125976562\n",
      "Loss: 140.32339477539062\n",
      "Loss: 142.50502014160156\n",
      "Loss: 141.643310546875\n",
      "Loss: 140.04742431640625\n",
      "Loss: 138.8493194580078\n",
      "Loss: 143.15045166015625\n",
      "Loss: 141.62828063964844\n",
      "Loss: 141.72296142578125\n",
      "Loss: 146.03030395507812\n",
      "Loss: 142.354248046875\n",
      "Loss: 141.87161254882812\n",
      "Loss: 141.49249267578125\n",
      "Loss: 143.68881225585938\n",
      "Loss: 144.58999633789062\n",
      "Loss: 140.65902709960938\n",
      "Loss: 138.35333251953125\n",
      "Loss: 136.150390625\n",
      "Loss: 137.59814453125\n",
      "Loss: 138.83126831054688\n",
      "Loss: 138.2044219970703\n",
      "Loss: 137.66714477539062\n",
      "Loss: 138.222900390625\n",
      "Loss: 136.30059814453125\n",
      "Loss: 136.3307647705078\n",
      "Loss: 136.50570678710938\n",
      "Loss: 138.65139770507812\n",
      "Loss: 139.154541015625\n",
      "Loss: 138.6858673095703\n",
      "Loss: 138.68870544433594\n",
      "Loss: 139.83609008789062\n",
      "Loss: 136.78564453125\n",
      "Loss: 135.81178283691406\n",
      "Loss: 135.68670654296875\n",
      "Loss: 135.3800506591797\n",
      "Loss: 135.24415588378906\n",
      "Loss: 134.7779541015625\n",
      "Loss: 135.85330200195312\n",
      "Loss: 134.914306640625\n",
      "Loss: 133.818115234375\n",
      "Loss: 133.80935668945312\n",
      "Loss: 134.09304809570312\n",
      "Loss: 133.94949340820312\n",
      "Loss: 132.94940185546875\n",
      "Loss: 134.182373046875\n",
      "Loss: 133.13980102539062\n",
      "Loss: 132.77505493164062\n",
      "Loss: 133.65109252929688\n",
      "Loss: 133.35708618164062\n",
      "Loss: 132.6845703125\n",
      "Loss: 132.9044189453125\n",
      "Loss: 134.05035400390625\n",
      "Loss: 133.21853637695312\n",
      "Loss: 133.95947265625\n",
      "Loss: 133.37847900390625\n",
      "Loss: 133.94503784179688\n",
      "Loss: 132.92332458496094\n",
      "Loss: 132.26513671875\n",
      "Loss: 132.67819213867188\n",
      "Loss: 132.49002075195312\n",
      "Loss: 133.26707458496094\n",
      "Loss: 132.81634521484375\n",
      "Loss: 132.6421661376953\n",
      "Loss: 133.00503540039062\n",
      "Loss: 132.5789794921875\n",
      "Loss: 132.26580810546875\n",
      "Loss: 131.69110107421875\n",
      "Loss: 132.04336547851562\n",
      "Loss: 132.10775756835938\n",
      "Loss: 132.09625244140625\n",
      "Loss: 132.3684844970703\n",
      "Loss: 131.63546752929688\n",
      "Loss: 131.9580535888672\n",
      "Loss: 131.83358764648438\n",
      "Loss: 133.14183044433594\n",
      "Loss: 132.31565856933594\n",
      "Loss: 132.65245056152344\n",
      "Loss: 132.01223754882812\n",
      "Loss: 131.97885131835938\n",
      "Loss: 132.95172119140625\n",
      "Loss: 132.38125610351562\n",
      "Loss: 133.0596466064453\n",
      "Loss: 133.1995849609375\n",
      "Loss: 132.8350830078125\n",
      "Loss: 133.03591918945312\n",
      "Loss: 132.82183837890625\n",
      "Loss: 133.0579071044922\n",
      "Loss: 132.51480102539062\n",
      "Loss: 132.77365112304688\n",
      "Loss: 132.42312622070312\n",
      "Loss: 131.97830200195312\n",
      "Loss: 132.81021118164062\n",
      "Loss: 132.39877319335938\n",
      "Loss: 135.22265625\n",
      "Loss: 134.28985595703125\n",
      "Loss: 133.67413330078125\n",
      "Loss: 133.50537109375\n",
      "Loss: 132.60855102539062\n",
      "Loss: 132.11268615722656\n",
      "Loss: 132.01681518554688\n",
      "Loss: 132.44708251953125\n",
      "Loss: 132.54220581054688\n",
      "Loss: 132.29840087890625\n",
      "Loss: 132.81317138671875\n",
      "Loss: 132.61709594726562\n",
      "Loss: 133.1998748779297\n",
      "Loss: 132.6758575439453\n",
      "Loss: 132.91558837890625\n",
      "Loss: 132.4461212158203\n",
      "Loss: 133.02102661132812\n",
      "Loss: 132.84825134277344\n",
      "Loss: 132.16082763671875\n",
      "Loss: 132.34774780273438\n",
      "Loss: 132.279541015625\n",
      "Loss: 131.65707397460938\n",
      "Loss: 132.09397888183594\n",
      "Loss: 132.01739501953125\n",
      "Loss: 132.9395751953125\n",
      "Loss: 132.21359252929688\n",
      "Loss: 132.511962890625\n",
      "Loss: 132.136962890625\n",
      "Loss: 132.30291748046875\n",
      "Loss: 133.53048706054688\n",
      "Loss: 132.89596557617188\n",
      "Loss: 132.89788818359375\n",
      "Loss: 132.56600952148438\n",
      "Loss: 132.47267150878906\n",
      "Loss: 133.3996124267578\n",
      "Loss: 133.06781005859375\n",
      "Loss: 132.27703857421875\n",
      "Loss: 133.57373046875\n",
      "Loss: 132.47439575195312\n",
      "Loss: 133.76629638671875\n",
      "Loss: 133.53807067871094\n",
      "Loss: 133.97518920898438\n",
      "Loss: 133.47991943359375\n",
      "Loss: 133.16708374023438\n",
      "Loss: 132.79730224609375\n",
      "Loss: 132.9061279296875\n",
      "Loss: 133.14846801757812\n",
      "Loss: 132.134521484375\n",
      "Loss: 132.13816833496094\n",
      "Loss: 131.50372314453125\n",
      "Loss: 133.77822875976562\n",
      "Loss: 134.30581665039062\n",
      "Loss: 134.8588104248047\n",
      "Loss: 133.80300903320312\n",
      "Loss: 132.85202026367188\n",
      "Loss: 132.98886108398438\n",
      "Loss: 133.33030700683594\n",
      "Loss: 132.98287963867188\n",
      "Loss: 133.01223754882812\n",
      "Loss: 133.16140747070312\n",
      "Loss: 134.1728515625\n",
      "Loss: 133.32107543945312\n",
      "Loss: 132.68907165527344\n",
      "Loss: 132.37045288085938\n",
      "Loss: 132.60885620117188\n",
      "Loss: 132.81439208984375\n",
      "Loss: 133.4563446044922\n",
      "Loss: 132.62307739257812\n",
      "Loss: 133.93907165527344\n",
      "Loss: 134.0583038330078\n",
      "Loss: 133.8389892578125\n",
      "Loss: 133.357421875\n",
      "Loss: 133.87832641601562\n",
      "Loss: 134.43484497070312\n",
      "Loss: 134.25457763671875\n",
      "Loss: 133.38461303710938\n",
      "Loss: 133.59120178222656\n",
      "Loss: 134.09019470214844\n",
      "Loss: 135.03317260742188\n",
      "Loss: 137.14437866210938\n",
      "Loss: 136.6890869140625\n",
      "Loss: 135.55477905273438\n",
      "Loss: 134.12998962402344\n",
      "Loss: 134.5450439453125\n",
      "Loss: 133.86703491210938\n",
      "Loss: 133.17604064941406\n",
      "Loss: 134.90341186523438\n",
      "Loss: 134.7357177734375\n",
      "Loss: 135.73265075683594\n",
      "Loss: 134.77577209472656\n",
      "Loss: 134.5757293701172\n",
      "Loss: 134.3527069091797\n",
      "Loss: 133.4178466796875\n",
      "Loss: 132.35955810546875\n",
      "Loss: 132.8627471923828\n",
      "Loss: 133.23162841796875\n",
      "Loss: 133.82052612304688\n",
      "Loss: 133.367431640625\n",
      "Loss: 134.05242919921875\n",
      "Loss: 134.8026580810547\n",
      "Loss: 134.25341796875\n",
      "Loss: 134.56146240234375\n",
      "Loss: 134.89486694335938\n",
      "Loss: 134.35031127929688\n",
      "Loss: 134.190673828125\n",
      "Loss: 133.88616943359375\n",
      "Loss: 133.99954223632812\n",
      "Loss: 134.03829956054688\n",
      "Loss: 133.51339721679688\n",
      "Loss: 133.90794372558594\n",
      "Loss: 134.204833984375\n",
      "Loss: 133.52975463867188\n",
      "Loss: 132.734619140625\n",
      "Loss: 134.17437744140625\n",
      "Loss: 133.67178344726562\n",
      "Loss: 133.5130157470703\n",
      "Loss: 133.95108032226562\n",
      "Loss: 133.62103271484375\n",
      "Loss: 134.72515869140625\n",
      "Loss: 134.67672729492188\n",
      "Loss: 134.4916534423828\n",
      "Loss: 134.80694580078125\n",
      "Loss: 134.40956115722656\n",
      "Loss: 134.02706909179688\n",
      "Loss: 133.89588928222656\n",
      "Loss: 133.55133056640625\n",
      "Loss: 133.7923126220703\n",
      "Loss: 133.73587036132812\n",
      "Loss: 133.55734252929688\n",
      "Loss: 133.97332763671875\n",
      "Loss: 134.1048583984375\n",
      "Loss: 134.66116333007812\n",
      "Loss: 134.17398071289062\n",
      "Loss: 133.41854858398438\n",
      "Loss: 133.44915771484375\n",
      "Loss: 132.69850158691406\n",
      "Loss: 132.92343139648438\n",
      "Loss: 132.35301208496094\n",
      "Loss: 132.08749389648438\n",
      "Loss: 132.54779052734375\n",
      "Loss: 131.8643341064453\n",
      "Loss: 133.27197265625\n",
      "Loss: 133.66729736328125\n",
      "Loss: 135.27896118164062\n",
      "Loss: 134.05078125\n",
      "Loss: 133.70248413085938\n",
      "Loss: 133.1383056640625\n",
      "Loss: 133.03350830078125\n",
      "Loss: 133.17031860351562\n",
      "Loss: 134.1404571533203\n",
      "Loss: 133.89898681640625\n",
      "Loss: 134.76304626464844\n",
      "Loss: 134.91688537597656\n",
      "Loss: 135.2110595703125\n",
      "Loss: 135.4434814453125\n",
      "Loss: 135.243408203125\n",
      "Loss: 134.82827758789062\n",
      "Loss: 134.38890075683594\n",
      "Loss: 134.63339233398438\n",
      "Loss: 134.53323364257812\n",
      "Loss: 134.76541137695312\n",
      "Loss: 135.154052734375\n",
      "Loss: 134.96774291992188\n",
      "Loss: 134.3880615234375\n",
      "Loss: 134.293701171875\n",
      "Loss: 133.52780151367188\n",
      "Loss: 134.07919311523438\n",
      "Loss: 133.50027465820312\n",
      "Loss: 134.04864501953125\n",
      "Loss: 134.4766387939453\n",
      "Loss: 134.0802001953125\n",
      "Loss: 133.55462646484375\n",
      "Loss: 133.09628295898438\n",
      "Loss: 132.85067749023438\n",
      "Loss: 132.85308837890625\n",
      "Loss: 134.41650390625\n",
      "Loss: 134.46400451660156\n",
      "Loss: 133.48782348632812\n",
      "Loss: 133.532470703125\n",
      "Loss: 133.33615112304688\n",
      "Loss: 133.75595092773438\n",
      "Loss: 133.36439514160156\n",
      "Loss: 133.880615234375\n",
      "Loss: 133.71212768554688\n",
      "Loss: 134.19107055664062\n",
      "Loss: 134.719482421875\n",
      "Loss: 134.4835205078125\n",
      "Loss: 134.24752807617188\n",
      "Loss: 135.54623413085938\n",
      "Loss: 137.6243438720703\n",
      "Loss: 138.17941284179688\n",
      "Loss: 135.94802856445312\n",
      "Loss: 136.47723388671875\n",
      "Loss: 136.38796997070312\n",
      "Loss: 135.6027069091797\n",
      "Loss: 136.35903930664062\n",
      "Loss: 143.93707275390625\n",
      "Loss: 137.9376983642578\n",
      "Loss: 137.61886596679688\n",
      "Loss: 139.52020263671875\n",
      "Loss: 143.05116271972656\n",
      "Loss: 138.80276489257812\n",
      "Loss: 138.81002807617188\n",
      "Loss: 139.6982879638672\n",
      "Loss: 137.6936492919922\n",
      "Loss: 138.2169189453125\n",
      "Loss: 137.02862548828125\n",
      "Loss: 137.02664184570312\n",
      "Loss: 137.0052032470703\n",
      "Loss: 136.77432250976562\n",
      "Loss: 135.5254669189453\n",
      "Loss: 135.55828857421875\n",
      "Loss: 135.14002990722656\n",
      "Loss: 134.30929565429688\n",
      "Loss: 134.10903930664062\n",
      "Loss: 134.8380584716797\n",
      "Loss: 134.46697998046875\n",
      "Loss: 135.00064086914062\n",
      "Loss: 134.10366821289062\n",
      "Loss: 134.96136474609375\n",
      "Loss: 133.99928283691406\n",
      "Loss: 135.5740966796875\n",
      "Loss: 136.54965209960938\n",
      "Loss: 135.83016967773438\n",
      "Loss: 136.0756378173828\n",
      "Loss: 134.53231811523438\n",
      "Loss: 135.06716918945312\n",
      "Loss: 135.79090881347656\n",
      "Loss: 135.793212890625\n",
      "Loss: 136.36065673828125\n",
      "Loss: 136.46905517578125\n",
      "Loss: 136.16464233398438\n",
      "Loss: 135.69815063476562\n",
      "Loss: 135.31942749023438\n",
      "Loss: 136.59835815429688\n",
      "Loss: 136.90817260742188\n",
      "Loss: 137.80992126464844\n",
      "Loss: 136.34373474121094\n",
      "Loss: 135.17034912109375\n",
      "Loss: 135.68118286132812\n",
      "Loss: 135.60577392578125\n",
      "Loss: 135.51177978515625\n",
      "Loss: 134.73794555664062\n",
      "Loss: 134.4329833984375\n",
      "Loss: 134.14755249023438\n",
      "Loss: 134.2684326171875\n",
      "Loss: 133.5855712890625\n",
      "Loss: 132.86318969726562\n",
      "Loss: 133.22836303710938\n",
      "Loss: 133.42758178710938\n",
      "Loss: 134.18563842773438\n",
      "Loss: 132.58419799804688\n",
      "Loss: 132.9578399658203\n",
      "Loss: 132.90847778320312\n",
      "Loss: 133.21263122558594\n",
      "Loss: 133.50982666015625\n",
      "Loss: 135.39324951171875\n",
      "Loss: 134.911376953125\n",
      "Loss: 134.64846801757812\n",
      "Loss: 134.41709899902344\n",
      "Loss: 133.60916137695312\n",
      "Loss: 133.8970184326172\n",
      "Loss: 133.63818359375\n",
      "Loss: 135.90110778808594\n",
      "Loss: 137.83770751953125\n",
      "Loss: 135.55307006835938\n",
      "Loss: 134.85629272460938\n",
      "Loss: 134.07614135742188\n",
      "Loss: 132.97776794433594\n",
      "Loss: 132.9832763671875\n",
      "Loss: 132.80731201171875\n",
      "Loss: 133.11293029785156\n",
      "Loss: 133.3164520263672\n",
      "Loss: 133.67776489257812\n",
      "Loss: 133.20550537109375\n",
      "Loss: 133.1859130859375\n",
      "Loss: 133.1475372314453\n",
      "Loss: 132.76461791992188\n",
      "Loss: 132.1744384765625\n",
      "Loss: 131.8332977294922\n",
      "Loss: 131.51260375976562\n",
      "Loss: 132.56805419921875\n",
      "Loss: 131.5616455078125\n",
      "Loss: 132.38641357421875\n",
      "Loss: 133.15396118164062\n",
      "Loss: 133.388916015625\n",
      "Loss: 132.68234252929688\n",
      "Loss: 132.42930603027344\n",
      "Loss: 132.4979248046875\n",
      "Loss: 133.1026611328125\n",
      "Loss: 133.43568420410156\n",
      "Loss: 133.77407836914062\n",
      "Loss: 134.23931884765625\n",
      "Loss: 133.90005493164062\n",
      "Loss: 134.4496307373047\n",
      "Loss: 135.41015625\n",
      "Loss: 134.61306762695312\n",
      "Loss: 134.88900756835938\n",
      "Loss: 134.8427276611328\n",
      "Loss: 133.8381805419922\n",
      "Loss: 132.69927978515625\n",
      "Loss: 133.17030334472656\n",
      "Loss: 132.76211547851562\n",
      "Loss: 133.05865478515625\n",
      "Loss: 134.47067260742188\n",
      "Loss: 134.1151123046875\n",
      "Loss: 134.4708251953125\n",
      "Loss: 134.2776641845703\n",
      "Loss: 133.59031677246094\n",
      "Loss: 133.24496459960938\n",
      "Loss: 133.36244201660156\n",
      "Loss: 133.32247924804688\n",
      "Loss: 133.69529724121094\n",
      "Loss: 133.77731323242188\n",
      "Loss: 134.12522888183594\n",
      "Loss: 133.594970703125\n",
      "Loss: 133.80465698242188\n",
      "Loss: 133.24264526367188\n",
      "Loss: 133.27516174316406\n",
      "Loss: 133.1833953857422\n",
      "Loss: 132.672607421875\n",
      "Loss: 133.3396453857422\n",
      "Loss: 133.32376098632812\n",
      "Loss: 134.27740478515625\n",
      "Loss: 134.05186462402344\n",
      "Loss: 133.864990234375\n",
      "Loss: 133.9060516357422\n",
      "Loss: 133.32559204101562\n",
      "Loss: 133.185302734375\n",
      "Loss: 133.18740844726562\n",
      "Loss: 133.28350830078125\n",
      "Loss: 133.365966796875\n",
      "Loss: 133.5238800048828\n",
      "Loss: 133.52957153320312\n",
      "Loss: 133.47537231445312\n",
      "Loss: 133.4664306640625\n",
      "Loss: 133.29763793945312\n",
      "Loss: 133.2189483642578\n",
      "Loss: 133.36007690429688\n",
      "Loss: 133.34063720703125\n",
      "Loss: 133.38211059570312\n",
      "Loss: 133.3527069091797\n",
      "Loss: 133.27044677734375\n",
      "Loss: 133.21441650390625\n",
      "Loss: 133.39041137695312\n",
      "Loss: 133.20477294921875\n",
      "Loss: 133.19996643066406\n",
      "Loss: 133.12896728515625\n",
      "Loss: 132.89405822753906\n",
      "Loss: 133.08349609375\n",
      "Loss: 132.36126708984375\n",
      "Loss: 132.17002868652344\n",
      "Loss: 132.4640350341797\n",
      "Loss: 132.9589080810547\n",
      "Loss: 133.09588623046875\n",
      "Loss: 133.2718505859375\n",
      "Loss: 133.46810913085938\n",
      "Loss: 132.6795196533203\n",
      "Loss: 133.10232543945312\n",
      "Loss: 133.11143493652344\n",
      "Loss: 132.53890991210938\n",
      "Loss: 132.64645385742188\n",
      "Loss: 133.40087890625\n",
      "Loss: 133.29150390625\n",
      "Loss: 133.02560424804688\n",
      "Loss: 133.75714111328125\n",
      "Loss: 133.00003051757812\n",
      "Loss: 133.515869140625\n",
      "Loss: 133.14205932617188\n",
      "Loss: 133.03590393066406\n",
      "Loss: 132.9618682861328\n",
      "Loss: 132.5336151123047\n",
      "Loss: 132.43646240234375\n",
      "Loss: 132.71435546875\n",
      "Loss: 133.68096923828125\n",
      "Loss: 133.2141876220703\n",
      "Loss: 133.14553833007812\n",
      "Loss: 132.96829223632812\n",
      "Loss: 133.17245483398438\n",
      "Loss: 133.196533203125\n",
      "Loss: 133.1551971435547\n",
      "Loss: 132.95831298828125\n",
      "Loss: 133.811767578125\n",
      "Loss: 133.1082763671875\n",
      "Loss: 132.97744750976562\n",
      "Loss: 132.80943298339844\n",
      "Loss: 133.03622436523438\n",
      "Loss: 133.29525756835938\n",
      "Loss: 133.263916015625\n",
      "Loss: 133.38160705566406\n",
      "Loss: 133.28668212890625\n",
      "Loss: 133.37203979492188\n",
      "Loss: 133.08139038085938\n",
      "Loss: 132.01925659179688\n",
      "Loss: 133.2633514404297\n",
      "Loss: 133.11630249023438\n",
      "Loss: 133.58163452148438\n",
      "Loss: 133.546142578125\n",
      "Loss: 134.3229522705078\n",
      "Loss: 134.22927856445312\n",
      "Loss: 134.25022888183594\n",
      "Loss: 134.34393310546875\n",
      "Loss: 133.84030151367188\n",
      "Loss: 133.80197143554688\n",
      "Loss: 133.5366973876953\n",
      "Loss: 133.22364807128906\n",
      "Loss: 133.45281982421875\n",
      "Loss: 133.2978515625\n",
      "Loss: 133.7713623046875\n",
      "Loss: 133.36538696289062\n",
      "Loss: 133.32431030273438\n",
      "Loss: 133.53952026367188\n",
      "Loss: 133.41278076171875\n",
      "Loss: 133.18505859375\n",
      "Loss: 133.36154174804688\n",
      "Loss: 133.01657104492188\n",
      "Loss: 132.9622344970703\n",
      "Loss: 132.97976684570312\n",
      "Loss: 133.10592651367188\n",
      "Loss: 133.3131866455078\n",
      "Loss: 133.16397094726562\n",
      "Loss: 133.09695434570312\n",
      "Loss: 133.10995483398438\n",
      "Loss: 133.2161407470703\n",
      "Loss: 133.3485107421875\n",
      "Loss: 133.2489471435547\n",
      "Loss: 133.04937744140625\n",
      "Loss: 133.1329345703125\n",
      "Loss: 133.08395385742188\n",
      "Loss: 133.09991455078125\n",
      "Loss: 133.01315307617188\n",
      "Loss: 133.18536376953125\n",
      "Loss: 133.03927612304688\n",
      "Loss: 133.36087036132812\n",
      "Loss: 133.23434448242188\n",
      "Loss: 133.91107177734375\n",
      "Loss: 134.1047821044922\n",
      "Loss: 133.9676513671875\n",
      "Loss: 133.76229858398438\n",
      "Loss: 133.3260498046875\n",
      "Loss: 133.17672729492188\n",
      "Loss: 133.0857391357422\n",
      "Loss: 133.18167114257812\n",
      "Loss: 133.2330780029297\n",
      "Loss: 133.7193603515625\n",
      "Loss: 133.29441833496094\n",
      "Loss: 133.22964477539062\n",
      "Loss: 133.1694793701172\n",
      "Loss: 133.04417419433594\n",
      "Loss: 133.18695068359375\n",
      "Loss: 133.3277130126953\n",
      "Loss: 133.53533935546875\n",
      "Loss: 133.37860107421875\n",
      "Loss: 133.56484985351562\n",
      "Loss: 133.72039794921875\n",
      "Loss: 134.14407348632812\n",
      "Loss: 133.62649536132812\n",
      "Loss: 133.212890625\n",
      "Loss: 133.22129821777344\n",
      "Loss: 133.03028869628906\n",
      "Loss: 132.99456787109375\n",
      "Loss: 133.8714141845703\n",
      "Loss: 133.63546752929688\n",
      "Loss: 133.4781494140625\n",
      "Loss: 133.5252685546875\n",
      "Loss: 133.30381774902344\n",
      "Loss: 133.77391052246094\n",
      "Loss: 134.10128784179688\n",
      "Loss: 134.80157470703125\n",
      "Loss: 135.68093872070312\n",
      "Loss: 135.94261169433594\n",
      "Loss: 135.7329864501953\n",
      "Loss: 135.36924743652344\n",
      "Loss: 135.17391967773438\n",
      "Loss: 135.36032104492188\n",
      "Loss: 135.3160400390625\n",
      "Loss: 134.14096069335938\n",
      "Loss: 133.83090209960938\n",
      "Loss: 138.246337890625\n",
      "Loss: 136.03451538085938\n",
      "Loss: 135.30929565429688\n",
      "Loss: 133.98614501953125\n",
      "Loss: 134.49285888671875\n",
      "Loss: 134.544189453125\n",
      "Loss: 134.63009643554688\n",
      "Loss: 134.99215698242188\n",
      "Loss: 134.70053100585938\n",
      "Loss: 134.02122497558594\n",
      "Loss: 133.19309997558594\n",
      "Loss: 133.0007781982422\n",
      "Loss: 132.70115661621094\n",
      "Loss: 133.2202911376953\n",
      "Loss: 134.09378051757812\n",
      "Loss: 133.8555908203125\n",
      "Loss: 134.5966033935547\n",
      "Loss: 134.59304809570312\n",
      "Loss: 134.49700927734375\n",
      "Loss: 134.78518676757812\n",
      "Loss: 134.09275817871094\n",
      "Loss: 133.7689208984375\n",
      "Loss: 133.79391479492188\n",
      "Loss: 134.74607849121094\n",
      "Loss: 133.56466674804688\n",
      "Loss: 135.51364135742188\n",
      "Loss: 134.4701690673828\n",
      "Loss: 134.18472290039062\n",
      "Loss: 134.09512329101562\n",
      "Loss: 133.90325927734375\n",
      "Loss: 133.785888671875\n",
      "Loss: 133.54742431640625\n",
      "Loss: 133.5040283203125\n",
      "Loss: 134.04966735839844\n",
      "Loss: 133.658447265625\n",
      "Loss: 133.86209106445312\n",
      "Loss: 134.3240966796875\n",
      "Loss: 134.38107299804688\n",
      "Loss: 134.87881469726562\n",
      "Loss: 135.05459594726562\n",
      "Loss: 134.7423095703125\n",
      "Loss: 135.06390380859375\n",
      "Loss: 135.61593627929688\n",
      "Loss: 135.80921936035156\n",
      "Loss: 136.20944213867188\n",
      "Loss: 136.74008178710938\n",
      "Loss: 137.2889404296875\n",
      "Loss: 136.8600311279297\n",
      "Loss: 136.36216735839844\n",
      "Loss: 135.69400024414062\n",
      "Loss: 136.19137573242188\n",
      "Loss: 135.08648681640625\n",
      "Loss: 135.5716552734375\n",
      "Loss: 135.47018432617188\n",
      "Loss: 135.82559204101562\n",
      "Loss: 135.29209899902344\n",
      "Loss: 135.68087768554688\n",
      "Loss: 136.223388671875\n",
      "Loss: 138.2779541015625\n",
      "Loss: 136.3946533203125\n",
      "Loss: 136.25643920898438\n",
      "Loss: 135.64700317382812\n",
      "Loss: 136.4860382080078\n",
      "Loss: 135.7823944091797\n",
      "Loss: 135.5666961669922\n",
      "Loss: 134.87899780273438\n",
      "Loss: 135.81309509277344\n",
      "Loss: 135.2999267578125\n",
      "Loss: 134.63888549804688\n",
      "Loss: 134.83680725097656\n",
      "Loss: 135.46200561523438\n",
      "Loss: 136.62863159179688\n",
      "Loss: 136.5612335205078\n",
      "Loss: 135.69073486328125\n",
      "Loss: 135.52511596679688\n",
      "Loss: 135.84829711914062\n",
      "Loss: 135.52316284179688\n",
      "Loss: 135.23007202148438\n",
      "Loss: 135.6253662109375\n",
      "Loss: 136.25323486328125\n",
      "Loss: 135.20724487304688\n",
      "Loss: 134.53640747070312\n",
      "Loss: 132.834228515625\n",
      "Loss: 133.1315460205078\n",
      "Loss: 134.31300354003906\n",
      "Loss: 134.89743041992188\n",
      "Loss: 135.1661376953125\n",
      "Loss: 135.29136657714844\n",
      "Loss: 137.7555694580078\n",
      "Loss: 136.27386474609375\n",
      "Loss: 136.06088256835938\n",
      "Loss: 136.03509521484375\n",
      "Loss: 135.30831909179688\n",
      "Loss: 135.53179931640625\n",
      "Loss: 135.40016174316406\n",
      "Loss: 134.2010498046875\n",
      "Loss: 135.4146270751953\n",
      "Loss: 135.88864135742188\n",
      "Loss: 136.08450317382812\n",
      "Loss: 135.3549041748047\n",
      "Loss: 135.62203979492188\n",
      "Loss: 134.6123046875\n",
      "Loss: 136.25888061523438\n",
      "Loss: 136.64077758789062\n",
      "Loss: 136.55191040039062\n",
      "Loss: 136.3668975830078\n",
      "Loss: 136.10812377929688\n",
      "Loss: 135.42584228515625\n",
      "Loss: 135.01437377929688\n",
      "Loss: 134.83799743652344\n",
      "Loss: 134.51695251464844\n",
      "Loss: 134.03233337402344\n",
      "Loss: 134.3799591064453\n",
      "Loss: 135.41378784179688\n",
      "Loss: 135.43341064453125\n",
      "Loss: 135.375732421875\n",
      "Loss: 135.37120056152344\n",
      "Loss: 135.28656005859375\n",
      "Loss: 135.22525024414062\n",
      "Loss: 135.070556640625\n",
      "Loss: 134.38470458984375\n",
      "Loss: 136.80535888671875\n",
      "Loss: 134.87762451171875\n",
      "Loss: 134.73182678222656\n",
      "Loss: 135.07626342773438\n",
      "Loss: 134.81134033203125\n",
      "Loss: 134.68992614746094\n",
      "Loss: 134.7671661376953\n",
      "Loss: 135.36619567871094\n",
      "Loss: 135.46841430664062\n",
      "Loss: 135.79739379882812\n",
      "Loss: 135.802734375\n",
      "Loss: 135.12831115722656\n",
      "Loss: 136.0792236328125\n",
      "Loss: 135.89434814453125\n",
      "Loss: 135.6513671875\n",
      "Loss: 135.36654663085938\n",
      "Loss: 135.57708740234375\n",
      "Loss: 135.4547119140625\n",
      "Loss: 135.79946899414062\n",
      "Loss: 135.37257385253906\n",
      "Loss: 135.43270874023438\n",
      "Loss: 134.42324829101562\n",
      "Loss: 137.26402282714844\n",
      "Loss: 134.93606567382812\n",
      "Loss: 135.8748779296875\n",
      "Loss: 136.57113647460938\n",
      "Loss: 136.33302307128906\n",
      "Loss: 137.01812744140625\n",
      "Loss: 137.87213134765625\n",
      "Loss: 136.52130126953125\n",
      "Loss: 136.7210693359375\n",
      "Loss: 136.6931610107422\n",
      "Loss: 137.12271118164062\n",
      "Loss: 137.36465454101562\n",
      "Loss: 139.58990478515625\n",
      "Loss: 137.4150848388672\n",
      "Loss: 137.45040893554688\n",
      "Loss: 137.7344970703125\n",
      "Loss: 137.1739959716797\n",
      "Loss: 137.37387084960938\n",
      "Loss: 136.30194091796875\n",
      "Loss: 135.48629760742188\n",
      "Loss: 136.2095947265625\n",
      "Loss: 135.78536987304688\n",
      "Loss: 136.26318359375\n",
      "Loss: 136.97052001953125\n",
      "Loss: 135.77685546875\n",
      "Loss: 137.7440185546875\n",
      "Loss: 136.53939819335938\n",
      "Loss: 136.15216064453125\n",
      "Loss: 137.48068237304688\n",
      "Loss: 136.32655334472656\n",
      "Loss: 136.15951538085938\n",
      "Loss: 135.5095977783203\n",
      "Loss: 136.62588500976562\n",
      "Loss: 137.0028533935547\n",
      "Loss: 137.9723358154297\n",
      "Loss: 138.6962127685547\n",
      "Loss: 138.90493774414062\n",
      "Loss: 138.63632202148438\n",
      "Loss: 137.83572387695312\n",
      "Loss: 136.86013793945312\n",
      "Loss: 138.70974731445312\n",
      "Loss: 138.9178924560547\n",
      "Loss: 139.266845703125\n",
      "Loss: 139.26092529296875\n",
      "Loss: 138.3079071044922\n",
      "Loss: 138.793701171875\n",
      "Loss: 155.86083984375\n",
      "Loss: 145.32476806640625\n",
      "Loss: 142.74212646484375\n",
      "Loss: 143.1802978515625\n",
      "Loss: 145.25831604003906\n",
      "Loss: 142.1790771484375\n",
      "Loss: 141.94903564453125\n",
      "Loss: 142.78782653808594\n",
      "Loss: 141.05487060546875\n",
      "Loss: 141.36402893066406\n",
      "Loss: 139.56939697265625\n",
      "Loss: 139.3553009033203\n",
      "Loss: 139.75848388671875\n",
      "Loss: 139.78707885742188\n",
      "Loss: 141.27919006347656\n",
      "Loss: 139.89857482910156\n",
      "Loss: 138.99644470214844\n",
      "Loss: 138.31951904296875\n",
      "Loss: 138.29339599609375\n",
      "Loss: 137.25302124023438\n",
      "Loss: 136.2975616455078\n",
      "Loss: 136.41827392578125\n",
      "Loss: 136.6837158203125\n",
      "Loss: 137.81565856933594\n",
      "Loss: 138.9880828857422\n",
      "Loss: 138.82212829589844\n",
      "Loss: 138.500732421875\n",
      "Loss: 139.54046630859375\n",
      "Loss: 139.26304626464844\n",
      "Loss: 138.8016357421875\n",
      "Loss: 140.22799682617188\n",
      "Loss: 140.40359497070312\n",
      "Loss: 140.0682373046875\n",
      "Loss: 141.43382263183594\n",
      "Loss: 139.29751586914062\n",
      "Loss: 139.25616455078125\n",
      "Loss: 139.22584533691406\n",
      "Loss: 138.96035766601562\n",
      "Loss: 137.90956115722656\n",
      "Loss: 136.89666748046875\n",
      "Loss: 138.16961669921875\n",
      "Loss: 138.74478149414062\n",
      "Loss: 137.71925354003906\n",
      "Loss: 136.7729949951172\n",
      "Loss: 135.65921020507812\n",
      "Loss: 136.18753051757812\n",
      "Loss: 136.0496826171875\n",
      "Loss: 136.184326171875\n",
      "Loss: 136.72093200683594\n",
      "Loss: 137.2711181640625\n",
      "Loss: 138.96804809570312\n",
      "Loss: 137.3277130126953\n",
      "Loss: 137.2701416015625\n",
      "Loss: 138.24635314941406\n",
      "Loss: 138.6439208984375\n",
      "Loss: 143.35459899902344\n",
      "Loss: 138.17926025390625\n",
      "Loss: 138.65570068359375\n",
      "Loss: 141.06149291992188\n",
      "Loss: 140.5318603515625\n",
      "Loss: 140.43203735351562\n",
      "Loss: 139.32061767578125\n",
      "Loss: 138.6978759765625\n",
      "Loss: 139.53726196289062\n",
      "Loss: 138.40975952148438\n",
      "Loss: 136.71450805664062\n",
      "Loss: 136.47817993164062\n",
      "Loss: 136.3387451171875\n",
      "Loss: 136.4326629638672\n",
      "Loss: 137.4340057373047\n",
      "Loss: 136.52117919921875\n",
      "Loss: 135.33987426757812\n",
      "Loss: 135.95748901367188\n",
      "Loss: 135.91070556640625\n",
      "Loss: 137.57073974609375\n",
      "Loss: 135.32984924316406\n",
      "Loss: 136.686767578125\n",
      "Loss: 143.37860107421875\n",
      "Loss: 142.95657348632812\n",
      "Loss: 138.225830078125\n",
      "Loss: 139.0836181640625\n",
      "Loss: 141.03427124023438\n",
      "Loss: 141.97784423828125\n",
      "Loss: 144.10818481445312\n",
      "Loss: 140.6436004638672\n",
      "Loss: 139.03488159179688\n",
      "Loss: 140.0814208984375\n",
      "Loss: 140.3131103515625\n",
      "Loss: 140.71286010742188\n",
      "Loss: 140.48141479492188\n",
      "Loss: 139.6986083984375\n",
      "Loss: 140.9503173828125\n",
      "Loss: 141.1822967529297\n",
      "Loss: 141.58090209960938\n",
      "Loss: 141.82086181640625\n",
      "Loss: 141.51170349121094\n",
      "Loss: 141.97198486328125\n",
      "Loss: 142.48387145996094\n",
      "Loss: 141.98764038085938\n",
      "Loss: 142.25277709960938\n",
      "Loss: 141.714111328125\n",
      "Loss: 141.45675659179688\n",
      "Loss: 141.416748046875\n",
      "Loss: 141.30764770507812\n",
      "Loss: 139.6024169921875\n",
      "Loss: 138.91404724121094\n",
      "Loss: 138.70266723632812\n",
      "Loss: 139.07778930664062\n",
      "Loss: 138.0482635498047\n",
      "Loss: 136.9527587890625\n",
      "Loss: 136.914794921875\n",
      "Loss: 136.42041015625\n",
      "Loss: 136.26461791992188\n",
      "Loss: 136.03408813476562\n",
      "Loss: 135.20883178710938\n",
      "Loss: 135.52854919433594\n",
      "Loss: 136.517578125\n",
      "Loss: 136.4459228515625\n",
      "Loss: 135.91505432128906\n",
      "Loss: 136.08499145507812\n",
      "Loss: 136.47879028320312\n",
      "Loss: 137.02142333984375\n",
      "Loss: 137.23260498046875\n",
      "Loss: 136.785400390625\n",
      "Loss: 140.7110595703125\n",
      "Loss: 139.15087890625\n",
      "Loss: 139.55532836914062\n",
      "Loss: 138.79974365234375\n",
      "Loss: 138.43106079101562\n",
      "Loss: 139.31698608398438\n",
      "Loss: 138.1136474609375\n",
      "Loss: 137.6873016357422\n",
      "Loss: 138.19651794433594\n",
      "Loss: 137.61358642578125\n",
      "Loss: 138.0028076171875\n",
      "Loss: 141.995361328125\n",
      "Loss: 138.89825439453125\n",
      "Loss: 139.05426025390625\n",
      "Loss: 139.66261291503906\n",
      "Loss: 139.71047973632812\n",
      "Loss: 139.6076202392578\n",
      "Loss: 139.83367919921875\n",
      "Loss: 139.49560546875\n",
      "Loss: 138.8515167236328\n",
      "Loss: 138.5023193359375\n",
      "Loss: 138.64031982421875\n",
      "Loss: 138.344970703125\n",
      "Loss: 137.44546508789062\n",
      "Loss: 137.54241943359375\n",
      "Loss: 137.03701782226562\n",
      "Loss: 136.96261596679688\n",
      "Loss: 137.01943969726562\n",
      "Loss: 137.90509033203125\n",
      "Loss: 137.22772216796875\n",
      "Loss: 138.05621337890625\n",
      "Loss: 137.83645629882812\n",
      "Loss: 138.53350830078125\n",
      "Loss: 138.19281005859375\n",
      "Loss: 139.064697265625\n",
      "Loss: 138.861083984375\n",
      "Loss: 137.68978881835938\n",
      "Loss: 137.58517456054688\n",
      "Loss: 137.36428833007812\n",
      "Loss: 137.167236328125\n",
      "Loss: 137.50709533691406\n",
      "Loss: 136.91983032226562\n",
      "Loss: 136.42300415039062\n",
      "Loss: 137.40585327148438\n",
      "Loss: 137.097900390625\n",
      "Loss: 137.9814910888672\n",
      "Loss: 138.32528686523438\n",
      "Loss: 138.59799194335938\n",
      "Loss: 138.26498413085938\n",
      "Loss: 138.5065155029297\n",
      "Loss: 138.48089599609375\n",
      "Loss: 138.33026123046875\n",
      "Loss: 138.203369140625\n",
      "Loss: 137.62649536132812\n",
      "Loss: 137.4874267578125\n",
      "Loss: 136.38938903808594\n",
      "Loss: 136.38937377929688\n",
      "Loss: 141.92974853515625\n",
      "Loss: 146.4551544189453\n",
      "Loss: 141.82383728027344\n",
      "Loss: 146.71771240234375\n",
      "Loss: 145.60333251953125\n",
      "Loss: 152.10340881347656\n",
      "Loss: 146.77757263183594\n",
      "Loss: 147.8438720703125\n",
      "Loss: 147.6170196533203\n",
      "Loss: 149.2213592529297\n",
      "Loss: 148.98715209960938\n",
      "Loss: 147.34027099609375\n",
      "Loss: 146.47378540039062\n"
     ]
    }
   ],
   "source": [
    "training_dataset = Data(input, output)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset,batch_size=256)\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for data in training_loader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss_size = loss(outputs, labels.to(device))\n",
    "        loss_size.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss_size.item()}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.7785e-01, -4.7360e+00,  5.8760e-01,  6.0990e-01,  7.5151e-01,\n",
      "         6.5074e-01,  6.2571e-01,  5.9408e-01,  6.0190e-01,  4.5404e-01,\n",
      "         1.0964e-01, -5.3351e-01,  9.9573e-02,  6.1304e-03,  2.3560e-02,\n",
      "         4.0875e-02,  8.3050e-02,  2.6840e-01,  2.3909e-01, -9.4117e-02,\n",
      "        -6.5678e-02,  1.3150e-01,  9.8387e-02,  9.9108e-02,  1.1590e-01,\n",
      "         3.9549e-02,  1.0780e-01,  2.8574e-01, -1.4106e-01, -1.4607e-01,\n",
      "         1.6567e-01, -2.0342e-01,  1.0628e-01,  1.2744e-02,  5.7347e-02,\n",
      "        -2.0378e-01,  1.4350e-01,  2.0978e-01, -1.7601e-01,  9.9285e-02,\n",
      "         1.2650e-01,  2.1908e-01, -1.2678e-01,  1.3858e-01, -1.1783e-01,\n",
      "         2.1543e-01,  5.1218e-02,  4.2658e-02, -4.9893e-02, -1.9746e-01,\n",
      "        -3.4787e-01, -1.4053e-01, -1.9032e-04, -7.1840e-02, -7.0026e-02,\n",
      "        -6.0925e-03, -1.7909e-02, -1.3124e-01, -3.1149e-02,  7.5167e-02,\n",
      "        -1.8330e-01,  4.2943e-02,  1.4329e-01,  1.7939e-02], device='mps:0')\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  val = torch.tensor(input[500], dtype=torch.float32).to(device)\n",
    "  answer = torch.tensor(output[500], dtype=torch.float32).to(device)\n",
    "\n",
    "  outputs = model(val)  # Get predictions from the model.\n",
    "  print(outputs.data)\n",
    "  print(answer)\n",
    "# print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
